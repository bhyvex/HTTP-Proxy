TODO
----

Here are all the features I plan to add to HTTP::Proxy, in the form of a
roadmap.

Things are listed by big features, since I seem to be unable to follow
my own roadmaps. :-/

* Windows support
  - the proxy breaks horribly under windows

* RFC support
  - Max-Forwards header
  - Answer to TRACE and OPTIONS request
  - support for chunked encoding
    => it's already in HTTP::Daemon, but the send_response
       require to have either the whole content (not practical
       for a proxy) or a callback (but we can't use both LWP::UA
       and HTTP::Daemon callback systems at the same time
       (some code will be cut and pasted)
    => should also use the send_redirect and send_error methods
       when possible

* Proxy Control
 - Control the proxy via specific URIs
 - Commands are called asking for http://proxy/cmd
   (the host part of the URI should be configurable)
   where cmd is a key to a dispatch table
 - Example commands
   * status => return some debug information
   * config => live config management (uses forms)
   * should be able to add more in inheriting classes
     => as an example, we could rewrite RGS's biscuit script
 - all this requires CGI/HTML, which I don't want to write or maintain
   the program should automatically create the configuration forms
   and handle the HTML/CSS stuff
 - configuration commands are handled by the parent process, so that
   future forked process obey the new configuration
 - how do subclassed HTTP::Proxy modules handle this?
 - can someone add his own commands by modifying either the class or
   the instance dispatch table?
 - the parent process preforks children, when a control URL is requested
   the child can write data to a special file/pipe and kill HUP the
   parent so that it reads the new information
 => control system probably means having a HTTP::Proxy::Control module...

* Special response handlers
 - must be able to break after processing a request and compute
   a specific response (other than the current fecth and filter)
 - must support: standard proxyied request, OPTIONS, TRACE, userdefined
 - these methods have the following prototype: method($req, ...)
 - they will be pushed by response_handler( GET => sub {} )
   (default: PROXY, OPTIONS, TRACE)
 => this can be useful if one want to create a caching proxy
    (though that might be complicated)
   

* Filters
 - filters cause several problems: the biggest being that, if they
   change the data size, they should update the Content-Lenght header
 - the filters to apply to a specific request or response can be
   determined from the headers, there is no need to go through the
   whole list of filters each time.
 - unfiltered response body should be send on the fly, while response
   that are to be filtered should be either filtered and sent
   (if one can determine that all the filters are isometric), or
   sent in a HTTP/1.0 fashion (Connection: close, and no
   Content-Length). Another solution is to support chunked transfer-coding
   with the client.
 - Must support the Content-Encoding: gzip headers
 - Accept-Encoding: content-coding tokens are identity, gzip, compress, deflate
 - add some "standard" filters, like a "recorder" for building web bots
 - we could add filter factories as modules (HTTP::Proxy::Filter::*)
   this could give some reflection capabilities to the system
   (like: naming the filters, giving the parameters used to create them
   and so on)
 - draft for a HTTP::Proxy::Filter class
   => name, args, description, code, buffer
   my $filter = HTTP::Proxy::Filter->new( code => $coderef, name =>...)
   or subclasses like: HTTP::Proxy::Filter::substitute->( LHS => qr//, RHS=>)
   
* Data flow
  I guess I'll change the way the responses are handled. The future should
  look like this :
  Request:
   * The client (C) sends a request
   * The proxy (P) receives the full request (headers+body)
   * P filters the request headers
   * P filters the request body
   * P runs the correct handler to get a response
     (this usually involves contacting a server, but not always)
  Response:
   * P receives the response in chunks (often)
   * 1st chunk is received
     - P filters the response headers
     - P computes the filter chain to run against each chunk
   * other chunks are passed to the filter chain
   * if any of the buffers used by the filters is not empty
     before-last chunk is sent with the remaining data
   * last chunk is sent (always empty)

  The system could even be more sophisticated by allowing one to
  indicate that a filter does not change the body size (s/foo/bar/g vs.
  s/foo/squigglewiggle/g). This way a Content-Length could be sent
  with the other headers.

* On the fly forwarding
 - handle big responses (>1Mb) with the LWP::UA callback system
   => short responses are concatenated together in a simple scalar,
      that gets filtered at the end
   => longer responses are filtered on the fly and sent as-is to the
      client. Since only chunks of the response are filtered, this can
      result in badly filtered data (I don't think we can do anything
      about it...)
 - this works, but we *have* a problem with Content-Length:
 - I suppose there is a problem also with gzip Transfered files, if
   we forward them as is, we can't filter them (need to think more
   about that)

* record sessions
 - the logs are not enough
 - this should probably be a "standard" filter
 - we want to be able to completely store a browsing session, probably
   as a tree, for example to use them as a base for WWW::Mechanize::Builder

* https and CONNECT support
 - proxy a https site, so as to filter and record, just as we do for
   http requests.
 - specific URLs ?
   => example: http://proxy/https://www.secure.com:443/index.html
   => This kind of stuff requires some content rewriting...

* support keep-alive with the client
 - make use of the keep-alive capabilities of LWP::UserAgent
   => one subprocess should accept a predefined number of requests
      (or all of them until it dies)
 - better handling of broken connection (from the client)

* other protocols
 - support for ftp
 - support for gopher 
   gopher://gopher.tc.umn.edu/
   gopher://marvel.loc.gov/

* special headers
 - support for hop by hop headers :
   => Connection, Keep-Alive, Proxy-Authenticate, Proxy-Authorization, TE,
      Trailers, Transfer-Encoding, Upgrade
 - handle big requests (see HTTP::Daemon::get_request documentation)

Better left to other modules
 - Browsing recording in a tree-like structure (thanks to Referer:)
   => maybe that should be left to a WWW::Mechanize::Builder module
 - become a POE component (OK, maybe that's another project)
   => POE::Component::?::?

Useful documentation:

http://www.stonehenge.com/merlyn/WebTechniques/col11.html
http://www.foad.org/~abigail/Perl/proxy.pl
http://rgarciasuarez.free.fr/perl/biscuit
http://www.jmarshall.com/tools/cgiproxy/

